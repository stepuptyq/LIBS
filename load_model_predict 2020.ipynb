{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b15bece139423399c6a69d658a8846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ea1596dbe844629dbec5104dca18bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.166s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7416a55f5b4970b50c0287f0ef4846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.163s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5f59305d864284986db663e8493347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.137s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a654d58caba6482f9fb76e9819619977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.139s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c66e065362c45f78d1dc89b25a65af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.110s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9103e3e595c421a8ee59f91c37c256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.134s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49be31eda2ce46758c147b10fbaaa02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.139s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41da21f7e03f41e8822d14085f64cb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.114s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "import random\n",
    "import xlrd\n",
    "import xlwt\n",
    "from scipy import optimize\n",
    "import scipy\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, mutual_info_classif\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn import manifold\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def baseline_als(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = scipy.sparse.csc_matrix(np.diff(np.eye(L), 2))\n",
    "    w = np.ones(L)\n",
    "    for i in range(niter):\n",
    "        W = scipy.sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = scipy.sparse.linalg.spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# position confirm 定位\n",
    "def wavelength_position(wl, wl_range):\n",
    "\n",
    "    output = []\n",
    "    for index in range(len(wl_range)):\n",
    "        output.append(wl.index(wl_range[index]))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 线性拟合\n",
    "def mean_spectra(x, n):\n",
    "    r_in, c_in = x.shape\n",
    "    r_out = int(r_in/n)\n",
    "    x_out = np.zeros([r_out, c_in])\n",
    "    for indexi in range(r_out):\n",
    "        x_out[indexi] = np.mean(x[indexi*n:(indexi+1)*n], axis=0)\n",
    "    return x_out\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 线性拟合\n",
    "def mean_spectra_new(x, n, s):\n",
    "    r_in, c_in = x.shape\n",
    "    r_out = int((r_in+1-n)/s)\n",
    "    x_out = np.zeros([r_out, c_in])\n",
    "    for indexi in range(r_out):\n",
    "        x_out[indexi] = np.mean(x[indexi*s:indexi*s+n], axis=0)\n",
    "    return x_out\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 背景扣除\n",
    "def correct_background(x, pixel_range, peak):\n",
    "    x = x.tolist()\n",
    "    back_ground_value = [x[i] for i in peak]\n",
    "    background_mean = np.mean(back_ground_value)\n",
    "    x_change = [xi-background_mean for xi in x[pixel_range[0]:pixel_range[1]]]\n",
    "    x[pixel_range[0]:pixel_range[1]] = x_change\n",
    "    x_out = np.array(x)\n",
    "    return x_out\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 自吸收矫正\n",
    "def correct_self_absoption(x, position_range):\n",
    "    x = x.tolist()\n",
    "    mid_position = position_range[0] + int((position_range[1]-position_range[0])/2)\n",
    "    x_choose = x[position_range[0]:position_range[1]]\n",
    "    max_value = max(x_choose)\n",
    "    x_choose1 = x[position_range[0]:mid_position]\n",
    "    x_choose2 = x[mid_position:position_range[1]]\n",
    "    start = x_choose1.index(max(x_choose1)) + position_range[0]\n",
    "    end = x_choose2.index(max(x_choose2)) + mid_position\n",
    "    x_change = [2*max_value-x for x in x[start:end]]\n",
    "    x[start:end] = x_change\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# label repeat\n",
    "def label_repeat(x, r):\n",
    "    x_output = x * np.ones([r, 1])\n",
    "    return x_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 线性拟合\n",
    "def f_1(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 除去重复\n",
    "def remove_duplicate(x):  # the imput should be the list style\n",
    "\n",
    "    x = x[:, 0].tolist()\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for i in x:\n",
    "\n",
    "        if i not in output:\n",
    "            output.append(i)\n",
    "\n",
    "    output = np.array(output).reshape(-1, 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 归一化同种样品的能量\n",
    "def normalize_single(spectra):\n",
    "\n",
    "    spectra_max = np.max(spectra, axis=1)\n",
    "\n",
    "    spectra_max = spectra_max.reshape(-1, 1)\n",
    "\n",
    "    spectra_mean = np.mean(spectra)\n",
    "\n",
    "    spectra_output = spectra*spectra_mean/spectra_max\n",
    "\n",
    "    return spectra_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 归一化同种样品的能量\n",
    "def normalize_single_1(spectra):\n",
    "\n",
    "    wh = np.where(spectra == np.max(spectra))\n",
    "\n",
    "    spectra_max = spectra[:, wh[1][0]]\n",
    "\n",
    "    spectra_max = spectra_max.reshape(-1, 1)\n",
    "\n",
    "    spectra_mean = np.mean(spectra_max)\n",
    "\n",
    "    spectra_output = spectra * spectra_mean / spectra_max\n",
    "\n",
    "    return spectra_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 归一化同种样品的能量\n",
    "def normalize_single_2(spectra):\n",
    "\n",
    "    spectra_mean_single = np.mean(spectra, axis=1)\n",
    "\n",
    "    spectra_mean_single = spectra_mean_single.reshape(-1, 1)\n",
    "\n",
    "    spectra_mean = np.mean(spectra)\n",
    "\n",
    "    spectra_output = spectra * spectra_mean / spectra_mean_single\n",
    "\n",
    "    return spectra_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 归一化样品的能量\n",
    "def normalize(spectra, group_set):\n",
    "\n",
    "    X_output = np.zeros((spectra.shape[0], spectra.shape[1]))\n",
    "\n",
    "    temp = remove_duplicate(group_set)\n",
    "\n",
    "    group_set = group_set[:, 0].tolist()\n",
    "\n",
    "    for indexj in range(len(temp)):\n",
    "\n",
    "        temp_order = [i for i, x in enumerate(group_set) if x == temp[indexj]]\n",
    "\n",
    "        X_temp = spectra[temp_order, :]\n",
    "\n",
    "        X_temp = normalize_single_1(X_temp)\n",
    "\n",
    "        X_output[temp_order] = X_temp\n",
    "\n",
    "    return X_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 归一化样品的能量\n",
    "def normalize1(spectra):\n",
    "\n",
    "    spectra_mean_single = np.mean(spectra, axis=1)\n",
    "\n",
    "    spectra_mean_single = spectra_mean_single.reshape(-1, 1)\n",
    "\n",
    "    spectra_mean = np.mean(spectra)\n",
    "\n",
    "    X_output = spectra * spectra_mean / spectra_mean_single\n",
    "\n",
    "    return X_output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 相对标准差\n",
    "def relative_standard_derivation(x_predict, x_te):  # the imput should be the list style\n",
    "\n",
    "    output = np.std((x_predict - x_te) / x_te) #* np.sqrt(len(x_te) / (len(x_te) - 1))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# 相对偏差\n",
    "def relative_error(x_predict, x_true):  # the input should be the list style\n",
    "\n",
    "    output = np.mean(abs(x_predict - x_true) / x_true)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# limited of detection\n",
    "def limited_of_detection(slope, rsd_calibration):\n",
    "\n",
    "    output = 3 * np.mean(rsd_calibration) / slope\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "#  BPNNet\n",
    "########################################################################################################################\n",
    "learning_rate_max_whole = 0.3\n",
    "learning_rate_min_whole = 0.2\n",
    "epochs_whole = 2000000\n",
    "batch_size_whole = 0\n",
    "threshold = 5e-7\n",
    "########################################################################################################################\n",
    "# active function\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "\n",
    "def relu_deriv(x):\n",
    "    x[x > 0] = 1.0\n",
    "    x[x <= 0] = 0.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x)*np.tanh(x)\n",
    "\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_derivative(x):\n",
    "    return logistic(x)*(1-logistic(x))\n",
    "\n",
    "\n",
    "def prelu(x):\n",
    "    return np.maximum(x, 0.0) + 0.25*np.minimum(x, 0.0)\n",
    "\n",
    "\n",
    "def prelu_deriv(x):\n",
    "    x[x > 0] = 1.0\n",
    "    x[x < 0] = 0.25\n",
    "    return x\n",
    "\n",
    "\n",
    "class BPNNet:\n",
    "\n",
    "    def __init__(self, layers, activation = [\"tanh\",\"tanh\"], batch_size=\"None\"):\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # active function\n",
    "        self.activation = activation\n",
    "\n",
    "        if self.activation[0] == 'logistic':\n",
    "\n",
    "            self.activation0 = logistic\n",
    "\n",
    "            self.activation0_deriv = logistic_derivative\n",
    "\n",
    "        elif self.activation[0] == 'tanh':\n",
    "\n",
    "            self.activation0 = tanh\n",
    "\n",
    "            self.activation0_deriv = tanh_deriv\n",
    "\n",
    "        elif self.activation[0] == 'relu':\n",
    "\n",
    "            self.activation0 = relu\n",
    "\n",
    "            self.activation0_deriv = relu_deriv\n",
    "\n",
    "        elif self.activation[0] == 'prelu':\n",
    "\n",
    "            self.activation0 = prelu\n",
    "\n",
    "            self.activation0_deriv = prelu_deriv\n",
    "\n",
    "        if self.activation[1] == 'logistic':\n",
    "\n",
    "            self.activation1 = logistic\n",
    "\n",
    "            self.activation1_deriv = logistic_derivative\n",
    "\n",
    "        elif self.activation[1] == 'tanh':\n",
    "\n",
    "            self.activation1 = tanh\n",
    "\n",
    "            self.activation1_deriv = tanh_deriv\n",
    "\n",
    "        elif self.activation[1] == 'relu':\n",
    "\n",
    "            self.activation1 = relu\n",
    "\n",
    "            self.activation1_deriv = relu_deriv\n",
    "\n",
    "        elif self.activation[1] == 'prelu':\n",
    "\n",
    "            self.activation1 = prelu\n",
    "\n",
    "            self.activation1_deriv = prelu_deriv\n",
    "\n",
    "        # batch_size\n",
    "        if batch_size == \"None\":\n",
    "\n",
    "            self.batch_size = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # random weights & weights updates\n",
    "        self.weights = []\n",
    "\n",
    "        self.weights.append((2*np.random.random((layers[0]+1,layers[1]))-1)*0.25)\n",
    "\n",
    "        for i in range(2,len(self.layers)):\n",
    "\n",
    "            self.weights.append((2*np.random.random((layers[i - 1],layers[i])) - 1) * 0.25)\n",
    "\n",
    "        # random weights & weights updates\n",
    "\n",
    "        self.updates = []\n",
    "\n",
    "        self.updates.append(np.zeros((self.layers[0] + 1, self.layers[1])))\n",
    "\n",
    "        for i in range(2,len(self.layers)):\n",
    "\n",
    "            self.updates.append(np.zeros((self.layers[i - 1], self.layers[i])))\n",
    "\n",
    "    def fit(self, X, y, learning_rate_max=0.3, learning_rate_min=0.1, epochs=2000000, error_threshold=1e-8): # learning_rate=0.2\n",
    "\n",
    "        # self.weights 的数据更新\n",
    "        self.weights[0] = (2*np.random.random((self.layers[0]+1,self.layers[1]))-1)*0.25\n",
    "\n",
    "        for i in range(2, len(self.layers)):\n",
    "            self.weights[i - 1] = (2*np.random.random((self.layers[i - 1],self.layers[i])) - 1) * 0.25\n",
    "\n",
    "        #print(self.weights)\n",
    "\n",
    "        # atlest_2d函数:确认X至少二位的矩阵\n",
    "\n",
    "        X = np.atleast_2d(X)\n",
    "\n",
    "        # 初始化矩阵全是1（行数，列数+1是为了有B这个偏向）\n",
    "\n",
    "        temp = np.ones([X.shape[0],X.shape[1]+1])\n",
    "\n",
    "        # 行全选，第一列到倒数第二列\n",
    "\n",
    "        temp[:,0:-1]=X\n",
    "\n",
    "        X = temp\n",
    "\n",
    "        # 真实值的y数组\n",
    "\n",
    "        y = np.array(y)\n",
    "\n",
    "        # batch_size 的mini_batch计算\n",
    "\n",
    "        batch_size = int(self.batch_size*X.shape[0])+1\n",
    "\n",
    "        print(batch_size)\n",
    "\n",
    "        # epoch 每一次循环的BP\n",
    "\n",
    "        for k in range(epochs):\n",
    "\n",
    "            learning_rate = learning_rate_max - (learning_rate_max-learning_rate_min)*k/epochs\n",
    "\n",
    "            # mini_batch 随机取出规定数目的序号list\n",
    "            order = random.sample(range(X.shape[0]), batch_size)\n",
    "\n",
    "            #print(order)\n",
    "\n",
    "            # self.updates 的数据更新\n",
    "            self.updates[0] = np.zeros((self.layers[0] + 1, self.layers[1]))\n",
    "\n",
    "            for i in range(2, len(self.layers)):\n",
    "                self.updates[i-1] = np.zeros((self.layers[i - 1], self.layers[i]))\n",
    "\n",
    "            # print(self.updates)\n",
    "\n",
    "            # mini_batch 梯度下降抽样\n",
    "            error = 0\n",
    "\n",
    "            for j in range(batch_size):\n",
    "\n",
    "                temp_j = order[j]\n",
    "\n",
    "                # 根据order产生的随机数数列，循环每次一个样本\n",
    "\n",
    "                a = [X[temp_j]]\n",
    "\n",
    "                a.append(self.activation0(np.dot(a[0], self.weights[0])))\n",
    "\n",
    "                a.append(self.activation1(np.dot(a[1], self.weights[1])))\n",
    "\n",
    "                # 向前传播，得到每个节点的输出结果\n",
    "\n",
    "                error_temp = y[temp_j] - a[-1]\n",
    "\n",
    "                error += error_temp\n",
    "\n",
    "            error = error/batch_size   # 最后一层错误率\n",
    "\n",
    "            deltas = [error * self.activation1_deriv(a[-1])]\n",
    "\n",
    "            deltas.append(deltas[-1].dot(self.weights[1].T) * self.activation0_deriv(a[1]))\n",
    "\n",
    "            deltas.reverse()\n",
    "\n",
    "            if abs(error) <= error_threshold:\n",
    "                print(k)\n",
    "                print(error)\n",
    "                break\n",
    "            elif k==epochs_whole-1:\n",
    "                print(k)\n",
    "                print(error)\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "\n",
    "                layer = np.atleast_2d(a[i])\n",
    "\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "\n",
    "                self.updates[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            for i in range(len(self.updates)):\n",
    "\n",
    "                self.weights[i] += self.updates[i]/batch_size\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        # x=np.array(x)\n",
    "        r1, c1 = x.shape\n",
    "\n",
    "        aa = np.ones(shape=[r1, 1])\n",
    "\n",
    "        for indexi in range(r1):\n",
    "\n",
    "            temp = np.ones(c1 + 1)\n",
    "\n",
    "            temp[0:-1] = x[indexi].T\n",
    "\n",
    "            a = temp\n",
    "\n",
    "            a = self.activation0(np.dot(a, self.weights[0]))\n",
    "\n",
    "            a = self.activation1(np.dot(a, self.weights[1]))\n",
    "\n",
    "            aa[indexi] = a\n",
    "\n",
    "        return (aa)\n",
    "\n",
    "    def load_weights(self, path_load):\n",
    "\n",
    "        '''''\n",
    "        worksheet = xlrd.open_workbook(path_load)\n",
    "        sheet_names = worksheet.sheet_names()\n",
    "        '''''\n",
    "        for indexi in range(len(self.weights)):\n",
    "\n",
    "            data = pd.read_excel(path_load, sheet_name=str(indexi),\n",
    "                                 header=None, skiprows=0)\n",
    "\n",
    "            self.weights[indexi] = np.array(data)\n",
    "\n",
    "\n",
    "\n",
    "    def save_weights(self, path_save):\n",
    "        '''''\n",
    "        workbook = xlwt.Workbook(encoding='ascii')\n",
    "\n",
    "        for indexi in range(len(self.weights)):\n",
    "            cmp = self.weights[indexi]\n",
    "            worksheet = workbook.add_sheet(str(indexi))\n",
    "            for i in range(len(cmp)):\n",
    "                for j, k in enumerate(cmp[i]):\n",
    "                    worksheet.write(i, j, k)\n",
    "\n",
    "        workbook.save(path_save)\n",
    "        '''''\n",
    "        with pd.ExcelWriter(path_save) as writer:\n",
    "            for indexi in range(len(self.weights)):\n",
    "\n",
    "                print(indexi)\n",
    "\n",
    "                data = self.weights[indexi]\n",
    "\n",
    "                data = pd.DataFrame(data)\n",
    "\n",
    "                data.to_excel(writer, sheet_name=str(indexi), index=False, header=False)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "for choose_element in tqdm_notebook(['SiO2','Fe2O3','Al2O3','CaO','MgO','K2O','Na2O','TiO2']):\n",
    "    for process in tqdm_notebook(['RawSpectrum']):\n",
    "        #choose_element = 'Fe2O3' #Na2O  SiO2 Fe2O3 Al2O3 CaO MgO  Na2O TiO2\n",
    "        date = '20191226'\n",
    "        try_num = 1\n",
    "        sub_path = os.path.join(date, choose_element+'_'+str(try_num))\n",
    "        main_path = r'D:\\data\\china_2020\\result\\BPNN results'\n",
    "        save_model_path = os.path.join(main_path, sub_path, process)\n",
    "        #predict_path = save_model_path\n",
    "        # repeat\n",
    "        kk = 6\n",
    "        rr = 10\n",
    "        # average parameter\n",
    "        mean_number = 180\n",
    "        step = 180\n",
    "        ########################################################################################################################\n",
    "        # Input Data\n",
    "        date_save = '20191226'\n",
    "        save_path_file = os.path.join(main_path, date_save, choose_element+'_'+str(try_num), process)\n",
    "        if not os.path.exists(save_path_file):\n",
    "            os.mkdir(save_path_file)\n",
    "        data_file_path = os.path.join(r'D:\\data\\china_2020\\coeffs_dwt\\coeffs_cut', choose_element)\n",
    "        menu_guide_path = r'D:\\data\\Menu_Guide-quantitative.xlsx'\n",
    "        choose_type = 'Soil_Type'\n",
    "        file_list = 'Spectra_Name'\n",
    "        sample_list = 'Spectra_Name'\n",
    "        data_flag = pd.read_excel(menu_guide_path)\n",
    "        file_names = np.array(data_flag[file_list])\n",
    "        concentration = np.array(data_flag[choose_element])\n",
    "        sample_type = np.array(data_flag[choose_type])\n",
    "        sample_name_unique = np.array(data_flag[sample_list])\n",
    "        train_test_set = np.array(data_flag['Train']).tolist()\n",
    "        type_list = data_flag[choose_type].drop_duplicates(keep='first').tolist()\n",
    "        flag_tr = False\n",
    "        flag_te = False\n",
    "        p = 150  #p可调\n",
    "        ########################################################################################################################\n",
    "        # train list\n",
    "        #train_position = [i for i, j in enumerate(train_test_set) if j == 1]\n",
    "        train_position = [1, 3, 12, 13, 21]\n",
    "        # test list\n",
    "        #test_position = [i for i, j in enumerate(train_test_set) if j == 0]\n",
    "        test_position = [1, 3, 12, 13, 21]\n",
    "        ########################################################################################################################\n",
    "        state = 'rock'\n",
    "        X_tr = None\n",
    "        for index_i in train_position:\n",
    "            temp_sample_type = type_list.index(sample_type[index_i])\n",
    "            temp_sample_name = sample_name_unique[index_i]\n",
    "            temp_concentration = concentration[index_i]\n",
    "            for j in range(8):\n",
    "                file = state+'_'+file_names[index_i]+'_'+process+'_'+str(j)+'.txt'\n",
    "                read_path = os.path.join(data_file_path, file)\n",
    "                if not os.path.exists(read_path):\n",
    "                    print(read_path)\n",
    "                    continue\n",
    "                data_cache = np.loadtxt(read_path)\n",
    "                #data_cache = data_cache.flatten()\n",
    "                if X_tr is None:\n",
    "                    X_tr = data_cache\n",
    "                    y_tr = temp_concentration\n",
    "                    #type_tr = temp_sample_type\n",
    "                    group_tr = index_i\n",
    "                else:\n",
    "                    X_tr = np.row_stack((X_tr, data_cache))\n",
    "                    y_tr = np.row_stack((y_tr, temp_concentration))\n",
    "                    #type_tr = np.row_stack((type_tr, temp_sample_type))\n",
    "                    group_tr = np.row_stack((group_tr, index_i))\n",
    "\n",
    "\n",
    "        '''\n",
    "        state = 'rock'\n",
    "        X_tr = None\n",
    "        for index_i in train_position:\n",
    "            temp_sample_type = type_list.index(sample_type[index_i])\n",
    "            temp_sample_name = sample_name_unique[index_i]\n",
    "            temp_concentration = concentration[index_i]\n",
    "            for i in range(3):\n",
    "                for j in range(8):\n",
    "                    file = state+'_'+file_names[index_i]+'_'+process+'_'+str(i+1)+'_'+str(j)+'.txt'\n",
    "                    read_path = os.path.join(data_file_path, file)\n",
    "                    if not os.path.exists(read_path):\n",
    "                        print(read_path)\n",
    "                        continue\n",
    "                    data_cache = np.loadtxt(read_path)\n",
    "                    data_cache = data_cache.flatten()\n",
    "                    if X_tr is None:\n",
    "                        X_tr = data_cache\n",
    "                        y_tr = temp_concentration\n",
    "                        type_tr = temp_sample_type\n",
    "                        group_tr = index_i\n",
    "                        sample_name_tr = temp_sample_name\n",
    "                        intensity = data_cache\n",
    "                    else:\n",
    "                        X_tr = np.row_stack((X_tr, data_cache))\n",
    "                        y_tr = np.row_stack((y_tr, temp_concentration))\n",
    "                        type_tr = np.row_stack((type_tr, temp_sample_type))\n",
    "                        sample_name_tr = np.row_stack((sample_name_tr, temp_sample_name))\n",
    "                        group_tr = np.row_stack((group_tr, index_i))\n",
    "        '''\n",
    "        ########################################################################################################################\n",
    "        # Te & Tr true concentration\n",
    "        group_tr_remove_duplicate = remove_duplicate(group_tr)\n",
    "        group_tr_temp = group_tr.reshape(1, -1)\n",
    "        group_tr_temp = group_tr_temp[0, :].tolist()\n",
    "        order_y_tr = [group_tr_temp.index(x) for x in group_tr_remove_duplicate]\n",
    "        y_tr_true = y_tr[order_y_tr,:]\n",
    "        ########################################################################################################################\n",
    "        # standard数据归一化\n",
    "        '''\n",
    "        min_max_scaler1 = preprocessing.MinMaxScaler()\n",
    "        X_tr = min_max_scaler1.fit_transform(X_tr)\n",
    "        X_te = min_max_scaler1.transform(X_te)\n",
    "        min_max_scaler2 = preprocessing.MinMaxScaler()\n",
    "        y_tr = min_max_scaler2.fit_transform(y_tr)\n",
    "        y_te = min_max_scaler2.transform(y_te)\n",
    "        min_max_scaler3 = preprocessing.MinMaxScaler()\n",
    "        group_tr_add = min_max_scaler3.fit_transform(group_tr)\n",
    "        group_te_add = min_max_scaler3.transform(group_te)\n",
    "        '''\n",
    "        ########################################################################################################################\n",
    "        # standard数据归一化for different states\n",
    "        min_max_scaler1_te = preprocessing.MinMaxScaler()\n",
    "        X_te = min_max_scaler1_te.fit_transform(X_tr)\n",
    "        min_max_scaler2_te = preprocessing.MinMaxScaler()\n",
    "        y_te = min_max_scaler2_te.fit_transform(y_tr)\n",
    "        min_max_scaler3_te = preprocessing.MinMaxScaler()\n",
    "        group_te_add = min_max_scaler3_te.fit_transform(group_tr)\n",
    "        ########################################################################################################################\n",
    "        r0, c0 = X_te.shape\n",
    "        predict_temp = np.empty(shape=[X_te.shape[0], kk * rr])\n",
    "        nn = BPNNet([c0, 5, 1], ['tanh','tanh'])\n",
    "        t0 = time()\n",
    "        for indexi in range(kk*rr):\n",
    "            nn.load_weights(save_model_path + r'\\weights'+str(indexi)+'.xlsx')\n",
    "            temp = nn.predict(X_te)\n",
    "            predict_temp[:, indexi] = temp[:, 0]\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        # 作图使用的calibration val && validation set的predicted value & errorbar\n",
    "        #  predicted value\n",
    "        predict_mean = np.mean(predict_temp, axis=1)\n",
    "        predict_std = np.std(predict_temp, axis=1)\n",
    "        num_cache = 0\n",
    "        sum_cache = 0\n",
    "        final_flag = True\n",
    "        spectra_per_sample = 8\n",
    "        pred_cache = np.zeros((spectra_per_sample))\n",
    "        for i in range(len(predict_mean)):\n",
    "            pred_cache[num_cache] = predict_mean[i]\n",
    "            num_cache += 1\n",
    "            sum_cache = sum_cache + predict_mean[i]\n",
    "            if num_cache == spectra_per_sample:\n",
    "                if final_flag:\n",
    "                    predict_final = sum_cache/spectra_per_sample\n",
    "                    true_final = y_te[i]\n",
    "                    std_final = np.std(pred_cache)\n",
    "                    final_flag = False\n",
    "                    num_cache = 0\n",
    "                    sum_cache = 0\n",
    "                else:\n",
    "                    predict_final = np.row_stack((predict_final, sum_cache/spectra_per_sample))\n",
    "                    true_final = np.row_stack((true_final, y_te[i]))\n",
    "                    std_final = np.row_stack((std_final, np.std(pred_cache)))\n",
    "                    num_cache = 0\n",
    "                    sum_cache = 0\n",
    "        '''\n",
    "        writer = pd.ExcelWriter(predict_path + r'\\predict.xlsx')\n",
    "        predict_results = np.column_stack((predict_mean, predict_std, y_te))\n",
    "        predict_results = pd.DataFrame(predict_results)\n",
    "        predict_results.to_excel(writer, header=['pred_mean', 'pred_std', 'true'], index=None)\n",
    "        writer.save()\n",
    "        '''\n",
    "        writer = pd.ExcelWriter(save_path_file + r'\\predict_final_all.xlsx')\n",
    "        predict_results = np.column_stack((predict_final, true_final, std_final))\n",
    "        predict_results = pd.DataFrame(predict_results)\n",
    "        predict_results.to_excel(writer, header=['pred_final', 'true_final', 'std_final'], index=None)\n",
    "        writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spectra\n",
    "########################################################################################################################\n",
    "for choose_element in tqdm_notebook(['SiO2','Fe2O3','Al2O3','CaO','MgO','K2O','Na2O','TiO2']):\n",
    "    for process in tqdm_notebook(['DenoisedSpectrum', 'SmoothedSpectrum']):\n",
    "        #choose_element = 'Fe2O3' #Na2O  SiO2 Fe2O3 Al2O3 CaO MgO  Na2O TiO2\n",
    "        date = '20191216'\n",
    "        try_num = 1\n",
    "        sub_path = os.path.join(date, choose_element)\n",
    "        main_path = r'D:\\data\\china_2020\\result\\BPNN results'\n",
    "        save_model_path = os.path.join(main_path, sub_path, process)\n",
    "        predict_path = save_model_path\n",
    "        # repeat\n",
    "        kk = 6\n",
    "        rr = 10\n",
    "        # average parameter\n",
    "        mean_number = 180\n",
    "        step = 180\n",
    "        ########################################################################################################################\n",
    "        # Input Data\n",
    "        save_path_file = save_model_path\n",
    "        data_file_path = os.path.join(r'D:\\data\\china_2020\\spectra_cut', choose_element)\n",
    "        menu_guide_path = r'D:\\data\\Menu_Guide-quantitative.xlsx'\n",
    "        choose_type = 'Soil_Type'\n",
    "        file_list = 'Spectra_Name'\n",
    "        sample_list = 'Spectra_Name'\n",
    "        data_flag = pd.read_excel(menu_guide_path)\n",
    "        file_names = np.array(data_flag[file_list])\n",
    "        concentration = np.array(data_flag[choose_element])\n",
    "        sample_type = np.array(data_flag[choose_type])\n",
    "        sample_name_unique = np.array(data_flag[sample_list])\n",
    "        train_test_set = np.array(data_flag['Train']).tolist()\n",
    "        type_list = data_flag[choose_type].drop_duplicates(keep='first').tolist()\n",
    "        flag_tr = False\n",
    "        flag_te = False\n",
    "        p = 150  #p可调\n",
    "        ########################################################################################################################\n",
    "        # train list\n",
    "        #train_position = [i for i, j in enumerate(train_test_set) if j == 1]\n",
    "        train_position = [1, 3, 12, 13, 21]\n",
    "        # test list\n",
    "        #test_position = [i for i, j in enumerate(train_test_set) if j == 0]\n",
    "        test_position = [1, 3, 12, 13, 21]\n",
    "        ########################################################################################################################\n",
    "        state = 'rock'\n",
    "        X_tr = None\n",
    "        for index_i in train_position:\n",
    "            temp_sample_type = type_list.index(sample_type[index_i])\n",
    "            temp_sample_name = sample_name_unique[index_i]\n",
    "            temp_concentration = concentration[index_i]\n",
    "            for j in range(8):\n",
    "                file = state+'_'+file_names[index_i]+'_'+process+'.txt'\n",
    "                read_path = os.path.join(data_file_path, file)\n",
    "                if not os.path.exists(read_path):\n",
    "                    print(read_path)\n",
    "                    continue\n",
    "                data_cache = np.loadtxt(read_path)\n",
    "                #data_cache = data_cache.flatten()\n",
    "                if X_tr is None:\n",
    "                    X_tr = data_cache\n",
    "                    y_tr = np.ones((8)).reshape(-1, 1) * temp_concentration\n",
    "                    #type_tr = temp_sample_type\n",
    "                    group_tr = np.ones((8)).reshape(-1, 1) * index_i\n",
    "                else:\n",
    "                    X_tr = np.row_stack((X_tr, data_cache))\n",
    "                    y_tr = np.row_stack((y_tr, np.ones((8)).reshape(-1, 1) * temp_concentration))\n",
    "                    #type_tr = np.row_stack((type_tr, temp_sample_type))\n",
    "                    group_tr = np.row_stack((group_tr, np.ones((8)).reshape(-1, 1) * index_i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
